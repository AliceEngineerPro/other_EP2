# 配置相关命令

## 1. 基础管理

### 1.1 修改主机名

```bash
hostnamectl set-hostname ${hostname}
```

### 1.2 关闭SELINUX

```bash
# 临时关闭
setenforce 0
# 重启生效
sed -i 's/SELINUX=enforcing/\SELINUX=disabled/' /etc/selinux/config
# 查看SELINUX状态  # enforcing代表打开，disabled代表关闭
getenforce
```

### 1.3 初始化系统环境

**1) 创建用户和目录(root下执行)**

```bash
groupadd -g 6000 apps  # -g: 指定用户组ID
useradd -s /bin/bash -g apps -d /home/app app  # -s:指定登录Shell, -g: 指定用户组, -d: 指定用户家目录
passwd app  # 为app用户修改密码
```

**2) 创建工作目录(root下执行)**

```bash
mkdir -p /data/projects/fate
mkdir -p /data/projects/install
chown -R app:apps /data/projects
```

**3) 安装依赖(root下执行)**

```bash
#centos
yum -y install gcc gcc-c++ make openssl-devel gmp-devel mpfr-devel libmpc-devel libaio numactl autoconf automake libtool libffi-devel snappy snappy-devel zlib zlib-devel bzip2 bzip2-devel lz4-devel libasan lsof sysstat telnet psmisc
#ubuntu
apt-get install -y gcc g++ make openssl supervisor libgmp-dev  libmpfr-dev libmpc-dev libaio1 libaio-dev numactl autoconf automake libtool libffi-dev libssl1.0.0 libssl-dev liblz4-1 liblz4-dev liblz4-1-dbg liblz4-tool  zlib1g zlib1g-dbg zlib1g-dev
cd /usr/lib/x86_64-linux-gnu
if [ ! -f "libssl.so.10" ];then
   ln -s libssl.so.1.0.0 libssl.so.10
   ln -s libcrypto.so.1.0.0 libcrypto.so.10
fi
```

**4) 修改linux文件句柄和用户进程数(root下执行)**

```bash
echo -e '* soft nofile 65536\n* hard nofile 65536' >> /etc/security/limits.conf
echo -e '* soft nproc unlimited' >> /etc/security/limits.d/20-nproc.conf
#文件句柄数, 不低于65535
ulimit -n
65535
#用户进程数, 不低于64000
ulimit -u
65535
```

## 2. FATE_依赖组件

- **服务器地址: 10.0.10.250, 10.0.10.251(Master, Slave))**

### 2.1 获取安装包(app用户下执行)

```bash
mkdir -p /data/projects/install
cd /data/projects/install
# 没有wget使用yum下载即可
# CentOS/RedHat
yum install wget -y
# Ubuntu
apt-get install wget -y
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/python-env-miniconda3-4.5.4.tar.gz
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/jdk-8u192-linux-x64.tar.gz
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/mysql-fate-8.0.28.tar.gz
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/openresty-1.17.8.2.tar.gz
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/pip-packages-fate-1.7.2.tar.gz
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/FATE_install_1.7.2_release.tar.gz
```

### 2.2 部署MySQL(app用户下执行)

**1) 部署安装**

```bash
#建立mysql根目录
mkdir -p /data/projects/fate/common/mysql
mkdir -p /data/projects/fate/data/mysql

#解压缩软件包
cd /data/projects/install
tar xf mysql-*.tar.gz
cd mysql
tar xf mysql-8.0.28.tar.gz -C /data/projects/fate/common/mysql

#配置设置
mkdir -p /data/projects/fate/common/mysql/mysql-8.0.28/{conf,run,logs}
cp service.sh /data/projects/fate/common/mysql/mysql-8.0.28/
cp my.cnf /data/projects/fate/common/mysql/mysql-8.0.28/conf

#初始化
cd /data/projects/fate/common/mysql/mysql-8.0.28/
./bin/mysqld --initialize --user=app --basedir=/data/projects/fate/common/mysql/mysql-8.0.28 --datadir=/data/projects/fate/data/mysql > logs/init.log 2>&1
cat ./logs/init.log |grep root@localhost
#注意输出信息中root@localhost:后的是mysql用户root的初始密码，需要记录，后面修改密码需要用到

#启动服务
cd /data/projects/fate/common/mysql/mysql-8.0.28/
nohup /data/projects/fate/common/mysql/mysql-8.0.28/bin/mysqld_safe --defaults-file=/data/projects/fate/common/mysql/mysql-8.0.28/conf/my.cnf --user=app >> /data/projects/fate/common/mysql/mysql-8.0.28/logs/mysqld.log 2>&1 &

#修改mysql root用户密码
cd /data/projects/fate/common/mysql/mysql-8.0.28/
./bin/mysqladmin -h 127.0.0.1 -P 3306 -S ./run/mysql.sock -u root -p password "fate_dev"
Enter Password:-->>输入root初始密码

#验证登陆
cd /data/projects/fate/common/mysql/mysql-8.0.28/
./bin/mysql -S ./run/mysql.sock -uroot -p'fate_dev'
```

**2)建库授权和业务配置**

```bash
cd /data/projects/fate/common/mysql/mysql-8.0.28/
./bin/mysql -u root -p -S ./run/mysql.sock
Enter Password:【fate_dev】

#创建fate_flow库
mysql>CREATE DATABASE IF NOT EXISTS fate_flow;

#创建远程用户和授权
mysql>CREATE USER 'fate'@'10.0.0.0/255.0.0.0' IDENTIFIED BY 'fate_dev';
mysql>GRANT ALL ON *.* TO 'fate'@'10.0.0.0/255.0.0.0';
mysql>flush privileges;

#校验
mysql>select User,Host from mysql.user;
mysql>show databases;
mysql>use eggroll_meta;
mysql>show tables;
mysql>select * from server_node;
```

### 2.3 部署Jdk(app用户下执行)

```bash
#创建jdk安装目录
mkdir -p /data/projects/fate/common/jdk
#解压缩
cd /data/projects/install
tar xzf jdk-8u192-linux-x64.tar.gz -C /data/projects/fate/common/jdk
cd /data/projects/fate/common/jdk
```

### 2.4 部署Python(app用户下执行)

```bash
#创建python虚拟化安装目录
mkdir -p /data/projects/fate/common/python

#安装miniconda3
cd /data/projects/install
tar xvf python-env-*.tar.gz
cd python-env
sh Miniconda3-4.5.4-Linux-x86_64.sh -b -p /data/projects/fate/common/miniconda3

#安装virtualenv和创建虚拟化环境
/data/projects/fate/common/miniconda3/bin/pip install virtualenv-20.0.18-py2.py3-none-any.whl -f . --no-index
/data/projects/fate/common/miniconda3/bin/virtualenv -p /data/projects/fate/common/miniconda3/bin/python3.6 --no-wheel --no-setuptools --no-download /data/projects/fate/common/python/venv
source /data/projects/fate/common/python/venv/bin/activate
pip install setuptools-42.0.2-py2.py3-none-any.whl

# 激活虚拟环境
source /data/projects/fate/common/python/venv/bin/activate
```

## 3. Hadoop_依赖组件

- **服务器地址: 10.0.10.252, 10.0.10.253, 10.0.10.254(Master, Slave1, Slave2)**

### 3.1 初始化服务器

**1) 配置sudo(root用户下执行)**

```bash
echo -e 'app ALL=(ALL) ALL\napp ALL=(ALL) NOPASSWD: ALL\nDefaults !env_reset' > /etc/sudoers.d/app
```

**2) 配置SSH免密(APP用户下执行)**

```bash
# 生成秘钥
su app
ssh-keygen -t rsa

# 合并id_rsa.pub
# 10.0.10.252
cat ~/.ssh/id_rsa.pub >> /home/app/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
scp ~/.ssh/authorized_keys app@10.0.10.253:/home/app/.ssh
# 10.0.10.253
cat ~/.ssh/id_rsa.pub >> /home/app/.ssh/authorized_keys
scp ~/.ssh/authorized_keys app@10.0.10.254:/home/app/.ssh
# 10.0.10.254
cat ~/.ssh/id_rsa.pub >> /home/app/.ssh/authorized_keys
# 覆盖历史文件(保证所有服务器authorized_keys文件保持一致)
scp ~/.ssh/authorized_keys app@IP:/home/app/.ssh
```

**3) 上传文件包, 解压(app用户下执行)**

```bash
# 上传文件到/home/app/part_hadoop
cd /home/app/part_hadoop
# 解压目录
mkdir -p /data/projects/common
# mkdir -p /data/projects/common/jdk
# 解压
tar xvf hadoop-2.8.5.tar.gz -C /data/projects/common
tar xvf scala-2.11.12.tar.gz -C /data/projects/common
tar xvf spark-2.4.1-bin-hadoop2.7.tar.gz -C /data/projects/common
tar xvf zookeeper-3.4.5.tar.gz -C /data/projects/common
tar xvf jdk-8u192-linux-x64.tar.gz -C /data/projects/common
# 文件重命名
cd /data/projects/common
mv hadoop-2.8.5 hadoop
mv scala-2.11.12 scala
mv spark-2.4.1-bin-hadoop2.7 spark
mv zookeeper-3.4.5 zookeeper
mv jdk1.8.0_192 jdk
```

**4) 配置环境变量(root用户下执行)**

**配置文件/etc/profile**

```bash
# 方式一
vim /etc/profile

export JAVA_HOME=/data/projects/common/jdk/
export PATH=$JAVA_HOME/bin:$PATH
export HADOOP_HOME=/data/projects/common/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export SPARK_HOME=/data/projects/common/spark
export PATH=$SPARK_HOME/bin:$PATH

# 方式二
echo -e "export JAVA_HOME=/data/projects/common/jdk/\nexport PATH=\$JAVA_HOME/bin:\$PATH\nexport HADOOP_HOME=/data/projects/common/hadoop\nexport PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin\nexport SPARK_HOME=/data/projects/common/spark\nexport PATH=\$SPARK_HOME/bin:\$PATH" >> /etc/profile
```

### 3.2 Zookeeper部署(app用户下执行)

- **zoo.cfg配置**

```bash
cd /data/projects/common/zookeeper/conf
mkdir -p /data/projects/common/zookeeper/data/{zookeeper,logs}
# 方式一
cat >> zoo.cfg << EOF
> tickTime=2000
> initLimit=10
> syncLimit=5
> dataDir=/data/projects/common/zookeeper/data/zookeeper
> dataLogDir=/data/projects/common/zookeeper/logs
> clientPort=2181
> maxClientCnxns=1000
> server.1=10.0.10.252:2888:3888
> server.2=10.0.10.253:2888:3888
> server.3=10.0.10.254:2888:3888
EOF

# 方式二
# 复制zoo.cfg模板
cp zoo_sample.cfg zoo.cfg
# 修改Zookeeper的数据存放路径
sed -i 's/dataDir=\/tmp\/zookeeper/\dataDir=\/data\/projects\/common\/zookeeper\/data\/zookeeper/' ./zoo.cfg
# 添加集群配置, 设置最大客户端连接数为1000, 配置Zookeeper的日志存放路径
echo -e 'server.1=10.0.10.252:2888:3888\nserver.2=10.0.10.253:2888:3888\nserver.3=10.0.10.254:2888:3888\nmaxClientCnxns=1000\ndataLogDir=/data/projects/common/zookeeper/logs' >> ./zoo.cfg
scp /data/projects/common/zookeeper/conf/zoo.cfg app@10.0.10.253:/data/projects/common/zookeeper/conf/
scp /data/projects/common/zookeeper/conf/zoo.cfg app@10.0.10.254:/data/projects/common/zookeeper/conf/
```

- **myid配置**

```bash
# Master
echo 1 > /data/projects/common/zookeeper/data/zookeeper/myid
# Slave1
echo 2 > /data/projects/common/zookeeper/data/zookeeper/myid
# Slave2
echo 3 > /data/projects/common/zookeeper/data/zookeeper/myid
```

- **启动Zookeeper**

```bash
nohup /data/projects/common/zookeeper/bin/zkServer.sh start >> /data/projects/common/zookeeper/data/logs/zookeeper.nohup.log 2>&1 &
```

### 3.3 Hadoop部署(app用户下执行)

```bash
# hadoop的配置文件路径
cd /data/projects/common/hadoop/etc/hadoop
```

**1) 配置Hadoop中JAVA_HOME的环境变量**

```bash
vi /data/projects/common/hadoop/etc/hadoop/hadoop-env.sh
vi /data/projects/common/hadoop/etc/hadoop/yarn-env.sh

# 环境配置
export JAVA_HOME=/data/project/common/jdk
```

**2) 配置core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml**

- <font size=6, color=red>core-site.xml</font>

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/projects/common/hadoop/tmp</value>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://fate-cluster</value>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec
        </value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <!-- <value>192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181</value> -->
        <value>10.0.10.252:2181,10.0.10.253:2181,10.0.10.254:2181</value>
    </property>
    <!-- Authentication for Hadoop HTTP web-consoles -->
    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
    </property>
    <property>
        <name>hadoop.http.authentication.type</name>
        <value>simple</value>
    </property>
    <property>
        <name>hadoop.http.authentication.token.validity</name>
        <value>3600</value>
    </property>
    <property>
        <name>hadoop.http.authentication.signature.secret.file</name>
        <value>/data/projects/commom/hadoop/etc/hadoop/hadoop-http-auth-signature-secret</value>
    </property>
    <property>
        <name>hadoop.http.authentication.cookie.domain</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>true</value>
    </property>
</configuration>
```

- <font size=6, color=red>hdfs-site.xml</font>

```bash
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.nameservices</name>
        <value>fate-cluster</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.fate-cluster</name>
        <value>nn1,nn2</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.fate-cluster.nn1</name>
        <value>10.0.10.252:9000</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.fate-cluster.nn1</name>
        <value>10.0.10.252:50070</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.fate-cluster.nn2</name>
        <value>10.0.10.253:9000</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.fate-cluster.nn2</name>
        <value>10.0.10.253:50070</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://10.0.10.252:8485;10.0.10.253:8485;10.0.10.254:8485/fate-cluster</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/projects/common/hadoop/data/journaldata</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/projects/common/hadoop/data/dfs/nn/local</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data/projects/common/hadoop/data/dfs/dn/local</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.fate-cluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/app/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>10000</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
        <value>NEVER</value>
    </property>
</configuration>
```

- <font size=6, color=red>mapred-site.xml</font>

```bash
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

- <font size=6, color=red>yarn-site.xml</font>

```bash
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>rmCluster</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>10.0.10.252:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>10.0.10.252</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>10.0.10.254</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>10.0.10.252:2181,10.0.10.253:2181,10.0.10.254:2181</value>
    </property>
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>

    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>

    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>20480</value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
        <value>97.0</value>
    </property>
</configuration>
```

**3) 新建目录**

```bash
cd  /data/projects/common/hadoop
mkdir ./tmp
mkdir -p ./data/dfs/nn/local
```

**4) 启动**

在Master, Slave1, Slave2下app用户下执行

```bash
hadoop-daemon.sh start journalnode
```

在Master下app用户下执行

```bash
hdfs namenode -format
hadoop-daemon.sh start namenode
```

在Slave1下app用户下操作

```bash
hdfs namenode -bootstrapStandby
```

在Master下app用户下执行

```bash
hdfs zkfc -formatZK
```

在Slave1下app用户下操作

```bash
hadoop-daemon.sh start namenode
```

在Master, Slave1下app用户下操作

```bash
hadoop-daemon.sh start zkfc
```

在Master, Slave1下app用户下操作

```bash
yarn-daemon.sh start resourcemanager
```

在Master, Slave1, Slave2下app用户下操作

```bash
yarn-daemon.sh start nodemanager
```

在Master, Slave1, Slave2下app用户下操作

```bash
hadoop-daemon.sh start datanode
```

**验证**

> http://10.0.10.252:50070 查看hadoop状态
> http://10.0.10.252:8088 查看yarn集群状态

### 3.4 Spark部署(app用户下执行)

```bash
cd /data/projects/common/spark/conf
cp ./slaves.template ./slaves && cp ./spark-defaults.conf.template ./spark-defaults.conf && cp ./spark-env.sh.template ./spark-env.sh
```

**1) 在slaves文件中添加Slave1, Slave2**

```bash
# 注释localhost
sed -i 's/localhost/\#localhost/g' /data/projects/common/spark/conf/slaves
echo -e "\nfate-H-S1\nfate-H-S2" >> /data/projects/common/spark/conf/slaves && cat /data/projects/common/spark/conf/slaves
```

**2) 编辑spark-defaults.conf配置文件**

```bash
# 方式一, 使用vim打开spark-defaults.conf配置文件添加以下内容
spark.master yarn
spark.eventLog.enabled true
spark.eventLog.dir hdfs://fate-cluster/tmp/spark/event
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.driver.memory 5g
spark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.yarn.jars hdfs://fate-cluster/tmp/spark/jars/\*.jar

# 方式二
echo -e "\nspark.master yarn\nspark.eventLog.enabled true\nspark.eventLog.dir hdfs://fate-cluster/tmp/spark/event\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.driver.memory 5g\nspark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\nspark.yarn.jars hdfs://fate-cluster/tmp/spark/jars/\\*.jar" >> /data/projects/common/spark/conf/spark-defaults.conf
```

**3) 编辑spark-env.sh配置文件**

```bash
# 方式一, 使用vim打开spark-env.sh配置文件添加以下内容
export JAVA_HOME=/data/projects/common/jdk/jdk1.8.0_192
export SCALA_HOME=/data/projects/common/scala
export HADOOP_HOME=/data/projects/common/hadoop
export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop
export SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://fate-cluster/tmp/spark/event"
export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=\$HADOOP_HOME/lib/native"
export LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:\${HADOOP_HOME}/lib/native
export PYSPARK_PYTHON=/data/projects/fate/common/python/venv/bin/python
export PYSPARK_DRIVER_PYTHON=/data/projects/fate/common/python/venv/bin/python

# 方式二
echo -e "\nexport JAVA_HOME=/data/projects/common/jdk\nexport SCALA_HOME=/data/projects/common/scala\nexport HADOOP_HOME=/data/projects/common/hadoop\nexport HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop\nexport SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=hdfs://fate-cluster/tmp/spark/event\"\nexport HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native\nexport HADOOP_OPTS=\"-Djava.library.path=\$HADOOP_HOME/lib/native\"\nexport LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:\$HADOOP_HOME/lib/native\nexport PYSPARK_PYTHON=/data/projects/fate/common/python/venv/bin/python\nexport PYSPARK_DRIVER_PYTHON=/data/projects/fate/common/python/venv/bin/python" >> /data/projects/common/spark/conf/spark-env.sh
```

**4) 启动Spark**

```bash
sh /data/projects/common/spark/sbin/start-all.sh
```

**5) 验证**

```bash
cd /data/projects/common/spark/jars
hdfs dfs -mkdir -p /tmp/spark/jars
hdfs dfs -mkdir -p /tmp/spark/event
hdfs dfs -put *jar /tmp/spark/jars
/data/projects/common/spark/bin/spark-shell --master yarn --deploy-mode client
```

## 4. Nginx_依赖组件

- **服务器地址: 10.0.10.250**

### 4.1 Nginx下载

```bash
mkdir -p /data/projects/install && cd /data/projects/install
wget https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/openresty-1.17.8.2.tar.gz
```

### 4.2 Nginx部署

```bash
cd /data/projects/install && tar xzf openresty-*.tar.gz && cd openresty-*
./configure --prefix=/data/projects/fate/proxy --with-luajit  --with-http_ssl_module --with-http_v2_module --with-stream --with-stream_ssl_module -j12
make && make install
```

### 4.3 配置修改

配置文件:/data/projects/fate/proxy/nginx/conf/nginx.conf此配置文件Nginx使用，配置服务基础设置以及lua代码，一般不需要修改, 若要修改，可以参考默认nginx.conf手工修改，修改完成后使用命令检测

```
# 检查nginx.conf配置文件
/data/projects/fate/proxy/nginx/sbin/nginx -t
```

```
#在目标服务器（192.168.0.1）app用户下修改执行
cat > /data/projects/fate/proxy/nginx/conf/route_table.yaml << EOF
default:
  proxy:
    - host: 192.168.0.2
      port: 9390
10000:
  proxy:
    - host: 192.168.0.1
      port: 9390
  fateflow:
    - host: 192.168.0.1
      port: 9360
9999:
  proxy:
    - host: 192.168.0.2
      port: 9390
  fateflow:
    - host: 192.168.0.2
      port: 9360
EOF

#在目标服务器（192.168.0.2）app用户下修改执行
cat > /data/projects/fate/proxy/nginx/conf/route_table.yaml << EOF
default:
  proxy:
    - host: 192.168.0.1
      port: 9390
10000:
  proxy:
    - host: 192.168.0.1
      port: 9390
  fateflow:
    - host: 192.168.0.1
      port: 9360
9999:
  proxy:
    - host: 192.168.0.2
      port: 9390
  fateflow:
    - host: 192.168.0.2
      port: 9360
EOF
```

### 4. 启动及日志模块

#### 4.1 启动服务

```
cd /data/projects/fate/proxy
./nginx/sbin/nginx -c /data/projects/fate/proxy/nginx/conf/nginx.conf
```

#### 4.2 日志目录

```
/data/projects/fate/proxy/nginx/logs
```
























